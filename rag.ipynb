{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0. Install dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121, https://pypi.ngc.nvidia.com\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp310-cp310-win_amd64.whl (2449.4 MB)\n",
      "     ---------------------------------------- 0.0/2.4 GB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.4 GB 93.0 MB/s eta 0:00:27\n",
      "      --------------------------------------- 0.0/2.4 GB 91.0 MB/s eta 0:00:27\n",
      "      --------------------------------------- 0.1/2.4 GB 90.8 MB/s eta 0:00:27\n",
      "     - -------------------------------------- 0.1/2.4 GB 92.0 MB/s eta 0:00:26\n",
      "     - -------------------------------------- 0.1/2.4 GB 91.7 MB/s eta 0:00:26\n",
      "     - -------------------------------------- 0.1/2.4 GB 90.4 MB/s eta 0:00:26\n",
      "     -- ------------------------------------- 0.1/2.4 GB 91.1 MB/s eta 0:00:26\n",
      "     -- ------------------------------------- 0.1/2.4 GB 91.6 MB/s eta 0:00:26\n",
      "     -- ------------------------------------- 0.2/2.4 GB 92.2 MB/s eta 0:00:25\n",
      "     --- ------------------------------------ 0.2/2.4 GB 92.0 MB/s eta 0:00:25\n",
      "     --- ------------------------------------ 0.2/2.4 GB 92.1 MB/s eta 0:00:25\n",
      "     --- ------------------------------------ 0.2/2.4 GB 92.2 MB/s eta 0:00:25\n",
      "     --- ------------------------------------ 0.2/2.4 GB 92.5 MB/s eta 0:00:24\n",
      "     ---- ----------------------------------- 0.3/2.4 GB 92.6 MB/s eta 0:00:24\n",
      "     ---- ----------------------------------- 0.3/2.4 GB 93.1 MB/s eta 0:00:24\n",
      "     ---- ----------------------------------- 0.3/2.4 GB 92.6 MB/s eta 0:00:24\n",
      "     ----- ---------------------------------- 0.3/2.4 GB 93.1 MB/s eta 0:00:23\n",
      "     ----- ---------------------------------- 0.3/2.4 GB 92.6 MB/s eta 0:00:23\n",
      "     ----- ---------------------------------- 0.4/2.4 GB 93.6 MB/s eta 0:00:23\n",
      "     ------ --------------------------------- 0.4/2.4 GB 94.1 MB/s eta 0:00:22\n",
      "     ------ --------------------------------- 0.4/2.4 GB 93.1 MB/s eta 0:00:23\n",
      "     ------ --------------------------------- 0.4/2.4 GB 93.1 MB/s eta 0:00:22\n",
      "     ------- -------------------------------- 0.4/2.4 GB 93.1 MB/s eta 0:00:22\n",
      "     ------- -------------------------------- 0.5/2.4 GB 93.6 MB/s eta 0:00:22\n",
      "     ------- -------------------------------- 0.5/2.4 GB 93.6 MB/s eta 0:00:22\n",
      "     -------- ------------------------------- 0.5/2.4 GB 93.1 MB/s eta 0:00:22\n",
      "     -------- ------------------------------- 0.5/2.4 GB 92.6 MB/s eta 0:00:21\n",
      "     -------- ------------------------------- 0.5/2.4 GB 92.1 MB/s eta 0:00:21\n",
      "     -------- ------------------------------- 0.5/2.4 GB 92.1 MB/s eta 0:00:21\n",
      "     --------- ------------------------------ 0.6/2.4 GB 93.1 MB/s eta 0:00:21\n",
      "     --------- ------------------------------ 0.6/2.4 GB 92.1 MB/s eta 0:00:21\n",
      "     --------- ------------------------------ 0.6/2.4 GB 91.1 MB/s eta 0:00:21\n",
      "     ---------- ----------------------------- 0.6/2.4 GB 91.1 MB/s eta 0:00:21\n",
      "     ---------- ----------------------------- 0.6/2.4 GB 91.1 MB/s eta 0:00:20\n",
      "     ---------- ----------------------------- 0.7/2.4 GB 91.6 MB/s eta 0:00:20\n",
      "     ----------- ---------------------------- 0.7/2.4 GB 91.1 MB/s eta 0:00:20\n",
      "     ----------- ---------------------------- 0.7/2.4 GB 90.6 MB/s eta 0:00:20\n",
      "     ----------- ---------------------------- 0.7/2.4 GB 90.6 MB/s eta 0:00:20\n",
      "     ----------- ---------------------------- 0.7/2.4 GB 90.6 MB/s eta 0:00:19\n",
      "     ------------ --------------------------- 0.8/2.4 GB 91.1 MB/s eta 0:00:19\n",
      "     ------------ --------------------------- 0.8/2.4 GB 91.6 MB/s eta 0:00:19\n",
      "     ------------ --------------------------- 0.8/2.4 GB 91.6 MB/s eta 0:00:19\n",
      "     ------------- -------------------------- 0.8/2.4 GB 91.6 MB/s eta 0:00:18\n",
      "     ------------- -------------------------- 0.8/2.4 GB 91.6 MB/s eta 0:00:18\n",
      "     ------------- -------------------------- 0.8/2.4 GB 92.1 MB/s eta 0:00:18\n",
      "     -------------- ------------------------- 0.9/2.4 GB 91.6 MB/s eta 0:00:18\n",
      "     -------------- ------------------------- 0.9/2.4 GB 91.1 MB/s eta 0:00:18\n",
      "     -------------- ------------------------- 0.9/2.4 GB 91.1 MB/s eta 0:00:18\n",
      "     -------------- ------------------------- 0.9/2.4 GB 91.1 MB/s eta 0:00:17\n",
      "     --------------- ------------------------ 0.9/2.4 GB 91.1 MB/s eta 0:00:17\n",
      "     --------------- ------------------------ 1.0/2.4 GB 91.6 MB/s eta 0:00:17\n",
      "     --------------- ------------------------ 1.0/2.4 GB 91.6 MB/s eta 0:00:17\n",
      "     ---------------- ----------------------- 1.0/2.4 GB 91.1 MB/s eta 0:00:17\n",
      "     ---------------- ----------------------- 1.0/2.4 GB 90.6 MB/s eta 0:00:16\n",
      "     ---------------- ----------------------- 1.0/2.4 GB 90.6 MB/s eta 0:00:16\n",
      "     ----------------- ---------------------- 1.0/2.4 GB 91.1 MB/s eta 0:00:16\n",
      "     ----------------- ---------------------- 1.1/2.4 GB 91.6 MB/s eta 0:00:16\n",
      "     ----------------- ---------------------- 1.1/2.4 GB 91.1 MB/s eta 0:00:15\n",
      "     ------------------ --------------------- 1.1/2.4 GB 91.6 MB/s eta 0:00:15\n",
      "     ------------------ --------------------- 1.1/2.4 GB 92.6 MB/s eta 0:00:15\n",
      "     ------------------ --------------------- 1.1/2.4 GB 92.6 MB/s eta 0:00:15\n",
      "     ------------------ --------------------- 1.2/2.4 GB 92.6 MB/s eta 0:00:14\n",
      "     ------------------- -------------------- 1.2/2.4 GB 92.6 MB/s eta 0:00:14\n",
      "     ------------------- -------------------- 1.2/2.4 GB 92.1 MB/s eta 0:00:14\n",
      "     ------------------- -------------------- 1.2/2.4 GB 91.6 MB/s eta 0:00:14\n",
      "     -------------------- ------------------- 1.2/2.4 GB 91.1 MB/s eta 0:00:14\n",
      "     -------------------- ------------------- 1.3/2.4 GB 92.1 MB/s eta 0:00:13\n",
      "     -------------------- ------------------- 1.3/2.4 GB 92.1 MB/s eta 0:00:13\n",
      "     --------------------- ------------------ 1.3/2.4 GB 92.1 MB/s eta 0:00:13\n",
      "     --------------------- ------------------ 1.3/2.4 GB 91.6 MB/s eta 0:00:13\n",
      "     --------------------- ------------------ 1.3/2.4 GB 91.6 MB/s eta 0:00:13\n",
      "     ---------------------- ----------------- 1.3/2.4 GB 92.1 MB/s eta 0:00:12\n",
      "     ---------------------- ----------------- 1.4/2.4 GB 92.1 MB/s eta 0:00:12\n",
      "     ---------------------- ----------------- 1.4/2.4 GB 91.6 MB/s eta 0:00:12\n",
      "     ---------------------- ----------------- 1.4/2.4 GB 91.6 MB/s eta 0:00:12\n",
      "     ----------------------- ---------------- 1.4/2.4 GB 92.1 MB/s eta 0:00:12\n",
      "     ----------------------- ---------------- 1.4/2.4 GB 92.1 MB/s eta 0:00:11\n",
      "     ----------------------- ---------------- 1.5/2.4 GB 92.1 MB/s eta 0:00:11\n",
      "     ------------------------ --------------- 1.5/2.4 GB 92.1 MB/s eta 0:00:11\n",
      "     ------------------------ --------------- 1.5/2.4 GB 92.6 MB/s eta 0:00:11\n",
      "     ------------------------ --------------- 1.5/2.4 GB 92.6 MB/s eta 0:00:11\n",
      "     ------------------------- -------------- 1.5/2.4 GB 93.1 MB/s eta 0:00:10\n",
      "     ------------------------- -------------- 1.6/2.4 GB 93.1 MB/s eta 0:00:10\n",
      "     ------------------------- -------------- 1.6/2.4 GB 93.1 MB/s eta 0:00:10\n",
      "     -------------------------- ------------- 1.6/2.4 GB 92.6 MB/s eta 0:00:10\n",
      "     -------------------------- ------------- 1.6/2.4 GB 92.6 MB/s eta 0:00:10\n",
      "     -------------------------- ------------- 1.6/2.4 GB 92.6 MB/s eta 0:00:09\n",
      "     -------------------------- ------------- 1.7/2.4 GB 93.7 MB/s eta 0:00:09\n",
      "     --------------------------- ------------ 1.7/2.4 GB 93.6 MB/s eta 0:00:09\n",
      "     --------------------------- ------------ 1.7/2.4 GB 93.1 MB/s eta 0:00:09\n",
      "     --------------------------- ------------ 1.7/2.4 GB 93.1 MB/s eta 0:00:08\n",
      "     ---------------------------- ----------- 1.7/2.4 GB 92.1 MB/s eta 0:00:08\n",
      "     ---------------------------- ----------- 1.7/2.4 GB 92.1 MB/s eta 0:00:08\n",
      "     ---------------------------- ----------- 1.8/2.4 GB 92.1 MB/s eta 0:00:08\n",
      "     ----------------------------- ---------- 1.8/2.4 GB 91.6 MB/s eta 0:00:08\n",
      "     ----------------------------- ---------- 1.8/2.4 GB 91.6 MB/s eta 0:00:08\n",
      "     ----------------------------- ---------- 1.8/2.4 GB 91.1 MB/s eta 0:00:07\n",
      "     ----------------------------- ---------- 1.8/2.4 GB 91.1 MB/s eta 0:00:07\n",
      "     ------------------------------ --------- 1.9/2.4 GB 92.1 MB/s eta 0:00:07\n",
      "     ------------------------------ --------- 1.9/2.4 GB 91.1 MB/s eta 0:00:07\n",
      "     ------------------------------ --------- 1.9/2.4 GB 91.1 MB/s eta 0:00:07\n",
      "     ------------------------------- -------- 1.9/2.4 GB 90.6 MB/s eta 0:00:06\n",
      "     ------------------------------- -------- 1.9/2.4 GB 91.1 MB/s eta 0:00:06\n",
      "     ------------------------------- -------- 2.0/2.4 GB 91.6 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 2.0/2.4 GB 92.6 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 2.0/2.4 GB 92.6 MB/s eta 0:00:05\n",
      "     -------------------------------- ------- 2.0/2.4 GB 93.1 MB/s eta 0:00:05\n",
      "     --------------------------------- ------ 2.0/2.4 GB 93.6 MB/s eta 0:00:05\n",
      "     --------------------------------- ------ 2.0/2.4 GB 93.6 MB/s eta 0:00:05\n",
      "     --------------------------------- ------ 2.1/2.4 GB 93.6 MB/s eta 0:00:05\n",
      "     ---------------------------------- ----- 2.1/2.4 GB 93.6 MB/s eta 0:00:04\n",
      "     ---------------------------------- ----- 2.1/2.4 GB 93.6 MB/s eta 0:00:04\n",
      "     ---------------------------------- ----- 2.1/2.4 GB 89.6 MB/s eta 0:00:04\n",
      "     ---------------------------------- ----- 2.1/2.4 GB 88.2 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 2.1/2.4 GB 88.2 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 2.2/2.4 GB 87.7 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 2.2/2.4 GB 87.3 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 2.2/2.4 GB 87.3 MB/s eta 0:00:03\n",
      "     ------------------------------------ --- 2.2/2.4 GB 87.7 MB/s eta 0:00:03\n",
      "     ------------------------------------ --- 2.2/2.4 GB 88.2 MB/s eta 0:00:03\n",
      "     ------------------------------------ --- 2.3/2.4 GB 88.2 MB/s eta 0:00:03\n",
      "     ------------------------------------- -- 2.3/2.4 GB 87.8 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 2.3/2.4 GB 87.8 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 2.3/2.4 GB 87.3 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 2.3/2.4 GB 87.7 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 2.4/2.4 GB 87.3 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 2.4/2.4 GB 93.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.4 GB 91.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.4 GB 91.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.4 GB 91.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.4 GB 91.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.4/2.4 GB 91.1 MB/s eta 0:00:00\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp310-cp310-win_amd64.whl (6.1 MB)\n",
      "     ---------------------------------------- 0.0/6.1 MB ? eta -:--:--\n",
      "     ---------------------------------------- 6.1/6.1 MB 92.1 MB/s eta 0:00:00\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp310-cp310-win_amd64.whl (4.1 MB)\n",
      "     ---------------------------------------- 0.0/4.1 MB ? eta -:--:--\n",
      "     ---------------------------------------- 4.1/4.1 MB 121.0 MB/s eta 0:00:00\n",
      "Collecting filelock (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "     ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 1.6/1.6 MB 86.6 MB/s eta 0:00:00\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "     ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "     ---------------------------------------- 6.2/6.2 MB 95.6 MB/s eta 0:00:00\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Downloading https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "     ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 536.2/536.2 kB ? eta 0:00:00\n",
      "Collecting numpy (from torchvision)\n",
      "  Downloading https://download.pytorch.org/whl/numpy-1.26.3-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "     ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 15.8/15.8 MB 99.7 MB/s eta 0:00:00\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Downloading https://download.pytorch.org/whl/pillow-10.2.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "     ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 2.6/2.6 MB 147.5 MB/s eta 0:00:00\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading https://download.pytorch.org/whl/MarkupSafe-2.1.5-cp310-cp310-win_amd64.whl (17 kB)\n",
      "Installing collected packages: mpmath, sympy, pillow, numpy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch, torchvision, torchaudio\n",
      "Successfully installed MarkupSafe-2.1.5 filelock-3.13.1 fsspec-2024.2.0 jinja2-3.1.3 mpmath-1.3.0 networkx-3.2.1 numpy-1.26.3 pillow-10.2.0 sympy-1.13.1 torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.10-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.2.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.0-py3-none-win_amd64.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: filelock in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from transformers) (24.2)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp310-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: torch>=1.11.0 in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from sentence-transformers) (2.5.1+cu121)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.5.2-cp310-cp310-win_amd64.whl.metadata (13 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.14.1-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: Pillow in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from sentence-transformers) (10.2.0)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-18.1.0-cp310-cp310-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.2.3-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.10-cp310-cp310-win_amd64.whl.metadata (8.0 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading SQLAlchemy-2.0.36-cp310-cp310-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting langchain-core<0.4.0,>=0.3.22 (from langchain)\n",
      "  Downloading langchain_core-0.3.22-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.2-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Downloading pydantic-2.10.3-py3-none-any.whl.metadata (172 kB)\n",
      "Collecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain)\n",
      "  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: psutil in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from accelerate) (6.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.8.0 in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from bitsandbytes) (4.12.2)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Downloading attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp310-cp310-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.2.1-cp310-cp310-win_amd64.whl.metadata (9.5 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.18.3-cp310-cp310-win_amd64.whl.metadata (71 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.22->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.12-cp310-none-win_amd64.whl.metadata (42 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.1 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading pydantic_core-2.27.1-cp310-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Downloading charset_normalizer-3.4.0-cp310-cp310-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.1.1-cp310-cp310-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: networkx in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting anyio (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading anyio-4.7.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.22->langchain)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Downloading transformers-4.47.0-py3-none-any.whl (10.1 MB)\n",
      "   ---------------------------------------- 0.0/10.1 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.8/10.1 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.2/10.1 MB 10.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.1/10.1 MB 13.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.1/10.1 MB 12.9 MB/s eta 0:00:00\n",
      "Downloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n",
      "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Downloading langchain-0.3.10-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/1.0 MB 24.1 MB/s eta 0:00:00\n",
      "Downloading accelerate-1.2.0-py3-none-any.whl (336 kB)\n",
      "Downloading bitsandbytes-0.45.0-py3-none-win_amd64.whl (68.5 MB)\n",
      "   ---------------------------------------- 0.0/68.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 5.0/68.5 MB 27.4 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 9.2/68.5 MB 22.8 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 16.3/68.5 MB 26.9 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 28.6/68.5 MB 35.5 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 31.7/68.5 MB 31.5 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 33.3/68.5 MB 27.8 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 38.0/68.5 MB 26.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 48.5/68.5 MB 30.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 52.2/68.5 MB 28.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 55.1/68.5 MB 27.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 55.8/68.5 MB 25.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 56.9/68.5 MB 23.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 60.8/68.5 MB 23.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 61.1/68.5 MB 22.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 61.1/68.5 MB 22.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 61.3/68.5 MB 19.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 61.6/68.5 MB 18.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 61.9/68.5 MB 17.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 61.9/68.5 MB 17.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 62.1/68.5 MB 15.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 62.4/68.5 MB 14.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 62.4/68.5 MB 14.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 62.7/68.5 MB 13.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 65.5/68.5 MB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 68.5/68.5 MB 13.7 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.11.10-cp310-cp310-win_amd64.whl (441 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading huggingface_hub-0.26.5-py3-none-any.whl (447 kB)\n",
      "Downloading langchain_core-0.3.22-py3-none-any.whl (409 kB)\n",
      "Downloading langchain_text_splitters-0.3.2-py3-none-any.whl (25 kB)\n",
      "Downloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading pyarrow-18.1.0-cp310-cp310-win_amd64.whl (25.1 MB)\n",
      "   ---------------------------------------- 0.0/25.1 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 9.7/25.1 MB 50.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 17.0/25.1 MB 44.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.1/25.1 MB 41.8 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.10.3-py3-none-any.whl (456 kB)\n",
      "Downloading pydantic_core-2.27.1-cp310-none-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 53.7 MB/s eta 0:00:00\n",
      "Downloading PyYAML-6.0.2-cp310-cp310-win_amd64.whl (161 kB)\n",
      "Downloading regex-2024.11.6-cp310-cp310-win_amd64.whl (274 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading safetensors-0.4.5-cp310-none-win_amd64.whl (285 kB)\n",
      "Downloading SQLAlchemy-2.0.36-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.1/2.1 MB 57.1 MB/s eta 0:00:00\n",
      "Downloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 66.5 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading pandas-2.2.3-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   ---------------------------- ----------- 8.1/11.6 MB 38.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 45.3 MB/s eta 0:00:00\n",
      "Downloading scikit_learn-1.5.2-cp310-cp310-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 5.5/11.0 MB 27.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 27.5 MB/s eta 0:00:00\n",
      "Downloading scipy-1.14.1-cp310-cp310-win_amd64.whl (44.8 MB)\n",
      "   ---------------------------------------- 0.0/44.8 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 5.0/44.8 MB 23.2 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 7.9/44.8 MB 19.5 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 11.3/44.8 MB 18.1 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 14.7/44.8 MB 18.1 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 18.6/44.8 MB 18.1 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 22.5/44.8 MB 18.3 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 23.6/44.8 MB 16.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 26.2/44.8 MB 16.0 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 30.4/44.8 MB 16.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 31.2/44.8 MB 15.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 36.4/44.8 MB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 41.4/44.8 MB 16.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 44.8/44.8 MB 17.6 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.5.0-cp310-cp310-win_amd64.whl (30 kB)\n",
      "Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Downloading charset_normalizer-3.4.0-cp310-cp310-win_amd64.whl (102 kB)\n",
      "Downloading frozenlist-1.5.0-cp310-cp310-win_amd64.whl (51 kB)\n",
      "Downloading greenlet-3.1.1-cp310-cp310-win_amd64.whl (298 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading multidict-6.1.0-cp310-cp310-win_amd64.whl (28 kB)\n",
      "Downloading orjson-3.10.12-cp310-none-win_amd64.whl (135 kB)\n",
      "Downloading propcache-0.2.1-cp310-cp310-win_amd64.whl (44 kB)\n",
      "Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Downloading yarl-1.18.3-cp310-cp310-win_amd64.whl (90 kB)\n",
      "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading anyio-4.7.0-py3-none-any.whl (93 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: pytz, xxhash, urllib3, tzdata, tqdm, threadpoolctl, tenacity, sniffio, scipy, safetensors, regex, pyyaml, pydantic-core, pyarrow, propcache, orjson, multidict, jsonpointer, joblib, idna, h11, greenlet, frozenlist, dill, charset-normalizer, certifi, attrs, async-timeout, annotated-types, aiohappyeyeballs, yarl, SQLAlchemy, scikit-learn, requests, pydantic, pandas, multiprocess, jsonpatch, httpcore, anyio, aiosignal, requests-toolbelt, huggingface-hub, httpx, bitsandbytes, aiohttp, tokenizers, langsmith, accelerate, transformers, langchain-core, datasets, sentence-transformers, langchain-text-splitters, evaluate, langchain\n",
      "Successfully installed SQLAlchemy-2.0.36 accelerate-1.2.0 aiohappyeyeballs-2.4.4 aiohttp-3.11.10 aiosignal-1.3.1 annotated-types-0.7.0 anyio-4.7.0 async-timeout-4.0.3 attrs-24.2.0 bitsandbytes-0.45.0 certifi-2024.8.30 charset-normalizer-3.4.0 datasets-3.1.0 dill-0.3.8 evaluate-0.4.3 frozenlist-1.5.0 greenlet-3.1.1 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 huggingface-hub-0.26.5 idna-3.10 joblib-1.4.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.10 langchain-core-0.3.22 langchain-text-splitters-0.3.2 langsmith-0.1.147 multidict-6.1.0 multiprocess-0.70.16 orjson-3.10.12 pandas-2.2.3 propcache-0.2.1 pyarrow-18.1.0 pydantic-2.10.3 pydantic-core-2.27.1 pytz-2024.2 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 requests-toolbelt-1.0.0 safetensors-0.4.5 scikit-learn-1.5.2 scipy-1.14.1 sentence-transformers-3.3.1 sniffio-1.3.1 tenacity-9.0.0 threadpoolctl-3.5.0 tokenizers-0.21.0 tqdm-4.67.1 transformers-4.47.0 tzdata-2024.2 urllib3-2.2.3 xxhash-3.5.0 yarl-1.18.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: numpy in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (1.26.3)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-win_amd64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: packaging in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from faiss-cpu) (24.2)\n",
      "Downloading faiss_cpu-1.9.0.post1-cp310-cp310-win_amd64.whl (13.8 MB)\n",
      "   ---------------------------------------- 0.0/13.8 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 9.4/13.8 MB 65.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 9.4/13.8 MB 65.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 9.4/13.8 MB 65.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 9.7/13.8 MB 13.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 9.7/13.8 MB 13.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 10.0/13.8 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 10.0/13.8 MB 9.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 10.2/13.8 MB 6.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 10.2/13.8 MB 6.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 10.5/13.8 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 10.5/13.8 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.7/13.8 MB 4.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 11.0/13.8 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 11.3/13.8 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.8/13.8 MB 4.6 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.9.0.post1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting absl-py\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: joblib in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: numpy in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from rouge_score) (1.26.3)\n",
      "Requirement already satisfied: six>=1.14.0 in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: colorama in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 82.9 MB/s eta 0:00:00\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (pyproject.toml): started\n",
      "  Building wheel for rouge_score (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24970 sha256=23f1582b2fec1aed69b42bc7aea6fd8ff75ff269a9af9c2600c629a6bb4cd617\n",
      "  Stored in directory: E:\\WindowsThings\\TEMP\\pip-ephem-wheel-cache-dfyh9zfx\\wheels\\5f\\dd\\89\\461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: click, absl-py, nltk, rouge_score\n",
      "Successfully installed absl-py-2.1.0 click-8.1.7 nltk-3.9.1 rouge_score-0.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from ipywidgets) (8.30.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.12 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: colorama in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: exceptiongroup in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.2.2)\n",
      "Requirement already satisfied: jedi>=0.16 in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack_data in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in d:\\programming\\rag pipeline\\rag-pipeline\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
      "Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.3/2.3 MB 14.7 MB/s eta 0:00:00\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 widgetsnbextension-4.0.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "%pip install transformers sentence-transformers datasets evaluate langchain accelerate bitsandbytes\n",
    "%pip install numpy faiss-cpu\n",
    "%pip install absl-py nltk rouge_score\n",
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import evaluate\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, concatenate_datasets, load_dataset\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, TextSplitter\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    GenerationConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Create config**\n",
    "Contains all the necessary parameters to easily modify the behaviour of RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cfg:\n",
    "    # General configs\n",
    "    SAVE_PATH: str = \"./\" \n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Dataset configs\n",
    "    WIKIPEDIA_DATASET_NAME: str = \"wikipedia\"\n",
    "    WIKIPEDIA_VERSION: str = \"20220301.en\"\n",
    "    WIKIPEDIA_SPLIT: str = \"train[:1%]\"\n",
    "    SHORT_RUN_DATASET_LEN: Optional[int] = 1000  # Use to short run, None: Use the whole dataset\n",
    "    NUM_PROC: int = max(os.cpu_count() - 4, 1)  # Number of processors to preprocess the dataset.\n",
    "\n",
    "    # Chunking configs\n",
    "    CHUNK_SIZE: int = 128  # Chunk size to split text. This is relevant if the context length is very limited.\n",
    "    CHUNK_OVERLAP: int = 0  # Overlap between chunks.\n",
    "    SEPARATORS: List[str] = [\"\\n\\n\", \"\\n\\t\", \"\\n\", \".\", \" \"]  # Separators to chunking.\n",
    "\n",
    "    # Retrieve configs\n",
    "    OVERRIDE_EXISTING_VECTOR_DB: bool = True  # Override the existing vector store or not.\n",
    "    RETRIEVE_DISTANCE_THRESHOLD: float = 0.6  # Only the best retrieved documents will present.\n",
    "    RETRIEVE_TOP_K: int = 5  # Number of documents to retrieve and add to the context.\n",
    "    RETRIEVE_MIN_K: int = 4  # Minimum number of documents to add to the context. The threshold is not taken into account.\n",
    "\n",
    "    # Embedding model configs\n",
    "    EMBEDDING_BATCH_SIZE: int = 128  # Embedding model batch size to encode text.\n",
    "    EMBEDDING_MODEL_NAME: str = \"multi-qa-distilbert-cos-v1\"\n",
    "\n",
    "    # Generation model config\n",
    "    GENERATION_MODEL_NAME: str = \"google/flan-t5-large\"  # encoder-decoder model, fine-tuned on instruction and Chain-of-thought datasets.\n",
    "    QUANTIZE_GENERATION_MODEL: bool = True  # Quantize the generation model to save memory and speed up inference. Loads in int8 precision.\n",
    "\n",
    "    # Generation strategies and configs\n",
    "    GENERATION_BATCH_SIZE: int = 8\n",
    "    MAX_NEW_TOKENS: int = 64  # max tokens to predict. The prompt does not count.\n",
    "    NUM_BEAMS: int = 1  # 1 is greedy search, take only output with the highest probability. This will make the output reproducible.\n",
    "    EARLY_STOPPING: bool = False  # Stop when EOS is predicted. Only make sense if num_beams > 1.\n",
    "    DO_SAMPLE: bool = False  # Random sample from the predicted tokens with high propability. False: deterministic True: random/undeterministic.\n",
    "    TEMPERATURE: Optional[float] = None  #  Constrols the random sample. If DO_SAMPLE false, then this parameter is irrelevant. lower value: output will be more predictable, higher value: creative\n",
    "    LENGTH_PENALTY: float = 1.0  # Controls the output length. If the output is too long then the penalty will be bigger, and short answers will be preferred.\n",
    "    GENERATION_TOP_K: Optional[float] = 50  # If DO_SAMPLE = True, choose only the k most likely tokens when sampling.\n",
    "    GENERATION_TOP_P: Optional[float] = 1.0  # If DO_SAMPLE = True, sort the predicted tokens by there probability and sum up while the sum is lower than TOP_P.\n",
    "\n",
    "    # Test configs\n",
    "    MAX_QUESTION_TO_EACH_TOPIC: int = 10  # Maximum question number to eval the RAG on it.\n",
    "    TEST_DATASET_NAME: str = \"rajpurkar/squad\"\n",
    "    TEST_COS_SIM_THRESHOLD: float = 0.6\n",
    "\n",
    "cfg = Cfg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create config object and save as a json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_to_dict(config_class: Cfg) -> Dict:\n",
    "    config_dict = {}\n",
    "    for attribute_name in dir(config_class):\n",
    "        if not attribute_name.startswith(\"__\") and not callable(\n",
    "            getattr(config_class, attribute_name)\n",
    "        ):\n",
    "            config_dict[attribute_name] = getattr(config_class, attribute_name)\n",
    "    return config_dict\n",
    "\n",
    "\n",
    "def write_json(data_to_write: Dict, file_name: str) -> None:\n",
    "    with open(cfg.SAVE_PATH + file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data_to_write, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "write_json(data_to_write=config_to_dict(cfg), file_name=\"cfg.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create embedding_model and tokenizer to the model. Set generation config params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.GENERATION_MODEL_NAME)\n",
    "embedding_model = SentenceTransformer(cfg.EMBEDDING_MODEL_NAME)\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=cfg.TEMPERATURE,\n",
    "    max_new_tokens=cfg.MAX_NEW_TOKENS,\n",
    "    num_beams=cfg.NUM_BEAMS,\n",
    "    early_stopping=cfg.EARLY_STOPPING,\n",
    "    do_sample=cfg.DO_SAMPLE,\n",
    "    length_penalty=cfg.LENGTH_PENALTY,\n",
    "    top_p=cfg.GENERATION_TOP_P,\n",
    "    top_k=cfg.GENERATION_TOP_K,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Load and preprocess data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(example: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Cleans up the text by removing unwanted characters and extra whitespace.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    text = example[\"text\"]\n",
    "    text = re.sub(r'(\\\\n)+', ' ', text).strip()  # Removes the newline characters\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s.,!?'\\';:(){}[\\]-]+\", \"\", text)  # Removes unwanted characters\n",
    "    text = re.sub(r\"(^|\\.\\s+)[^a-zA-Z0-9]+\", \"\", text)  # Keep only relevant characters in front of the text\n",
    "    example[\"text\"] = text\n",
    "    return example\n",
    "\n",
    "\n",
    "def remove_irrelevant_sections(example: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Removes irrelevant sections from an article.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    sections = re.split(\n",
    "        r\"\\b(References|External links|Further reading|See also|Notes|Bibliography|Sources|External references|Related topics|Image credits|Historical context)\\b\",\n",
    "        example[\"text\"],\n",
    "        flags=re.IGNORECASE,\n",
    "    )\n",
    "    example[\"text\"] = sections[0].strip()\n",
    "    return example\n",
    "\n",
    "\n",
    "def split_document(example: Dict, text_splitter: TextSplitter) -> Dict:\n",
    "    \"\"\"\n",
    "    Splits the article into chunks.\n",
    "    \"\"\"\n",
    "    from langchain.schema import Document as LangchainDocument\n",
    "\n",
    "    doc = LangchainDocument(\n",
    "        page_content=str(example[\"text\"]), metadata={\"title\": example[\"title\"]}\n",
    "    )\n",
    "    chunks = text_splitter.split_documents([doc])\n",
    "    return {\n",
    "        \"text\": [chunk.page_content for chunk in chunks],\n",
    "        \"title\": [chunk.metadata for chunk in chunks],\n",
    "    }\n",
    "\n",
    "\n",
    "def split_to_chunks(dataset: Dataset) -> Dataset:\n",
    "    \"\"\"\n",
    "    Splits long articles into chunks. This prevent too long context in prediction.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        tokenizer=tokenizer,\n",
    "        chunk_size=cfg.CHUNK_SIZE,\n",
    "        chunk_overlap=cfg.CHUNK_OVERLAP,\n",
    "        strip_whitespace=True,\n",
    "        separators=cfg.SEPARATORS,\n",
    "    )\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        partial(split_document, text_splitter=text_splitter),\n",
    "        batched=True,\n",
    "        batch_size=1,\n",
    "        remove_columns=dataset.column_names,\n",
    "        num_proc=cfg.NUM_PROC,\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_and_preprocess_data() -> List[str]:\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the dataset. Saves relevant topics. Returns with the preprcoessed text list.\n",
    "    \"\"\"\n",
    "    print(\"Loading Wikipedia dataset.\")\n",
    "    wikipedia = load_dataset(\n",
    "        cfg.WIKIPEDIA_DATASET_NAME,\n",
    "        cfg.WIKIPEDIA_VERSION,\n",
    "        split=cfg.WIKIPEDIA_SPLIT,\n",
    "    )\n",
    "\n",
    "    if cfg.SHORT_RUN_DATASET_LEN is not None:\n",
    "        wikipedia = wikipedia.select(range(cfg.SHORT_RUN_DATASET_LEN))\n",
    "\n",
    "    write_json(data_to_write={\"topics\": wikipedia[\"title\"]}, file_name=\"topics.json\")\n",
    "\n",
    "    print(\"Removing irrelevant sections from dataset.\")\n",
    "    wikipedia = wikipedia.map(remove_irrelevant_sections, num_proc=cfg.NUM_PROC)\n",
    "\n",
    "    print(\"Splitting long texts into chunks.\")\n",
    "    wikipedia = split_to_chunks(dataset=wikipedia)\n",
    "\n",
    "    print(\"Removing unnecessary tokens.\")\n",
    "    wikipedia = wikipedia.map(preprocess_text, num_proc=cfg.NUM_PROC)\n",
    "\n",
    "    print(\"Deleting duplicates.\")\n",
    "    wikipedia = pd.DataFrame(wikipedia).drop_duplicates(\"text\", ignore_index=True)\n",
    "\n",
    "    wikipedia_texts = wikipedia[\"text\"]\n",
    "    print(f\"Total documents: {len(wikipedia_texts)}\")\n",
    "\n",
    "    return wikipedia_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Embedding generation**\n",
    "Depending on the batch size, the embedding may be a little bit different for the same input.\n",
    "\n",
    "For example:\n",
    "```python\n",
    "e1 = embedding_model.encode(\n",
    "    [\"foo\"] * 128, show_progress_bar=True, convert_to_numpy=True, batch_size=128\n",
    ")\n",
    "e2 = embedding_model.encode(\n",
    "    [\"foo\"], show_progress_bar=True, convert_to_numpy=True, batch_size=1\n",
    ")\n",
    "(e1[0] == e2[0]).all() == False\n",
    "```\n",
    "These differences are due to numerical floating point differences.\n",
    "BatchNorm and LayerNorm are sensitive to batch size as these layers normalize the output, so the distibution can be slightly different.\n",
    "\n",
    "**Numerical consistency check:**\n",
    "```python\n",
    "from numpy.testing import assert_allclose\n",
    "assert_allclose(e1[0], e2[0], rtol=1e-3, atol=0)\n",
    "```\n",
    "Here \"rtol\" is the relative tolerance, and \"atol\" is the absolute tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(texts: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate embedding from texts.\n",
    "    \"\"\"\n",
    "    embeddings = embedding_model.encode(\n",
    "        texts,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        batch_size=cfg.EMBEDDING_BATCH_SIZE,\n",
    "    )\n",
    "    return np.array(embeddings, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cosine similarity focuses on the direction rather than the distance between vectors.\n",
    "\n",
    "Pros:\n",
    "- The magnitude of the vectors may vary significantly, but cosine similarity focuses on direction, so magnitude is less critical\n",
    "- If the features of a vector are scaled, cosine similarity remains unchanged because it is based on the cosine of the angle and not the vectors length\n",
    "- Directly measure how aligned two vectors are in their feature spac\n",
    "- 1: perfect alignment (high similarity)\n",
    "- 0: orthogonal to each other (no similarity)\n",
    "- -1: exactly opposite in direction (anti-similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_db(embeddings: np.ndarray) -> faiss.Index:\n",
    "    \"\"\"\n",
    "    Create vector db. The vector db uses cosine similarity.\n",
    "    \"\"\"\n",
    "    embeddings = embeddings.astype(\"float32\")\n",
    "\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    vector_db = faiss.IndexFlatIP(embeddings.shape[1])  # cosine similarity\n",
    "    vector_db.add(embeddings)\n",
    "\n",
    "    return vector_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Create retrieval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRAGRetrieval:\n",
    "    def __init__(self, vector_db: faiss.Index, texts: List[str]):\n",
    "        super().__init__()\n",
    "        self.vector_db = vector_db\n",
    "        self.texts = texts\n",
    "\n",
    "    def retrieve(\n",
    "        self,\n",
    "        query: str | List[str],\n",
    "        top_k: int,\n",
    "        retrieve_distance_threshold: float = 0.0,\n",
    "        retrieve_min_k: int = 1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Retrieve the relevant documents based on cosine similarity.\n",
    "        \"\"\"\n",
    "        query = [query] if isinstance(query, str) else query\n",
    "        query_embedding = embedding_model.encode(\n",
    "            query, convert_to_numpy=True, precision=\"float32\"\n",
    "        )\n",
    "\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        similarity_scores, indices = self.vector_db.search(query_embedding, top_k)\n",
    "\n",
    "        valid_mask = similarity_scores > retrieve_distance_threshold\n",
    "        valid_indices = [\n",
    "            indices[row][valid_mask[row]] for row in range(indices.shape[0])\n",
    "        ]\n",
    "\n",
    "        for idx in range(len(valid_indices)):\n",
    "            if len(valid_indices[idx]) < retrieve_min_k:\n",
    "                valid_indices[idx] = np.array(indices[idx, :retrieve_min_k])\n",
    "\n",
    "        retrieved_texts = [[self.texts[idx] for idx in idxs] for idxs in valid_indices]\n",
    "        return retrieved_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Generate answer functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRAGModel:\n",
    "    def __init__(\n",
    "        self, generation_model: AutoModelForSeq2SeqLM, tokenizer: AutoTokenizer\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.generation_model = generation_model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def _generate_outputs(self, input: Dict, max_input_length: int):\n",
    "        \"\"\"\n",
    "        Generates the output from the input. \n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            input[\"prompt\"],\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=max_input_length,\n",
    "            padding=True,\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "        inputs = inputs.to(self.generation_model.device)\n",
    "\n",
    "        if generation_config is not None:\n",
    "            outputs = self.generation_model.generate(\n",
    "                inputs, generation_config=generation_config\n",
    "            )\n",
    "        else:\n",
    "            outputs = self.generation_model.generate(inputs)\n",
    "\n",
    "        answer = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        return answer\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: str | List[str],\n",
    "        generation_config: Optional[GenerationConfig] = None,\n",
    "        batch_size: int = 8,\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate answer based on the prompt.\n",
    "        \"\"\"\n",
    "        max_new_tokens = (\n",
    "            generation_config.max_new_tokens\n",
    "            if generation_config is not None\n",
    "            and generation_config.max_new_tokens is not None\n",
    "            else 20\n",
    "        )\n",
    "        max_input_length = self.tokenizer.model_max_length - max_new_tokens\n",
    "\n",
    "        if len(prompt) > batch_size:\n",
    "            from torch.utils.data import DataLoader\n",
    "\n",
    "            dataloader = DataLoader(\n",
    "                Dataset.from_dict({\"prompt\": prompt}), batch_size=batch_size\n",
    "            )\n",
    "            answers = []\n",
    "            for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "                answer = self._generate_outputs(\n",
    "                    input=batch, max_input_length=max_input_length\n",
    "                )\n",
    "                answers.extend(answer)\n",
    "        else:\n",
    "            answers = self._generate_outputs(\n",
    "                input={\"prompt\": prompt}, max_input_length=max_input_length\n",
    "            )\n",
    "\n",
    "        return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRAGPipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "        retrieval: CustomRAGRetrieval,\n",
    "        model: CustomRAGModel,\n",
    "        tokenizer: AutoTokenizer,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.retrieval = retrieval\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def _create_prompt(\n",
    "        self, context: List[str], question: str | List[str]\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Creates the final prompt.\n",
    "        \"\"\"\n",
    "        if isinstance(question, str):\n",
    "            return [f\"Context: {context} Question: {question} Answer: \"]\n",
    "        return [\n",
    "            f\"Context: {c} Question: {q} Answer: \" for c, q in zip(context, question)\n",
    "        ]\n",
    "\n",
    "    def generate_answer(\n",
    "        self,\n",
    "        question: str | List[str],\n",
    "        top_k: int = 5,\n",
    "        retrieve_distance_threshold: float = 0.0,\n",
    "        retrieve_min_k: int = 1,\n",
    "        generation_config: Optional[GenerationConfig] = None,\n",
    "        batch_size: int = 8,\n",
    "    ) -> Dict | List[Dict]:\n",
    "        \"\"\"\n",
    "        Generates answer to the given question.\n",
    "        \"\"\"\n",
    "        retrieved_texts = self.retrieval.retrieve(\n",
    "            query=question,\n",
    "            top_k=top_k,\n",
    "            retrieve_distance_threshold=retrieve_distance_threshold,\n",
    "            retrieve_min_k=retrieve_min_k,\n",
    "        )\n",
    "        context = [\" \".join(rt) for rt in retrieved_texts]\n",
    "        prompt = self._create_prompt(context=context, question=question)\n",
    "        answers = self.model.generate(\n",
    "            prompt=prompt, generation_config=generation_config, batch_size=batch_size\n",
    "        )\n",
    "        if isinstance(question, str):\n",
    "            return {\n",
    "                \"answer\": answers[0],\n",
    "                \"context\": retrieved_texts,\n",
    "                \"prompt\": prompt[0],\n",
    "                \"question\": question,\n",
    "            }\n",
    "        return [\n",
    "            {\"answer\": a, \"retrieved_texts\": rt, \"prompt\": p, \"question\": q}\n",
    "            for a, rt, p, q in zip(answers, retrieved_texts, prompt, question)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Create pipeline for text data embedding and retrieval**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and prerocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Wikipedia dataset.\n",
      "Removing irrelevant sections from dataset.\n",
      "Splitting long texts into chunks.\n",
      "Removing unnecessary tokens.\n",
      "Deleting duplicates.\n",
      "Total documents: 40034\n"
     ]
    }
   ],
   "source": [
    "texts = load_and_preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading or creating a vector database from the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new vector database.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac2f975446a4a1cbc94b59895ae5aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index_path = cfg.SAVE_PATH + \"vector_db.index\"\n",
    "if os.path.exists(index_path) and cfg.OVERRIDE_EXISTING_VECTOR_DB:\n",
    "    print(f\"Loading existing vector database from {index_path}.\")\n",
    "    vector_db = faiss.read_index(index_path)\n",
    "else:\n",
    "    print(\"Creating a new vector database.\")\n",
    "    embeddings = generate_embeddings(texts)\n",
    "    vector_db = create_vector_db(embeddings)\n",
    "    faiss.write_index(vector_db, index_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generation_model() -> AutoModelForSeq2SeqLM:\n",
    "    if cfg.QUANTIZE_GENERATION_MODEL:\n",
    "        quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        return AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            cfg.GENERATION_MODEL_NAME,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "    return AutoModelForSeq2SeqLM.from_pretrained(cfg.GENERATION_MODEL_NAME,)\n",
    "\n",
    "generation_model = get_generation_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_retrieval = CustomRAGRetrieval(vector_db=vector_db, texts=texts)\n",
    "rag_model = CustomRAGModel(generation_model=generation_model, tokenizer=tokenizer)\n",
    "rag_pipeline = CustomRAGPipeline(retrieval=rag_retrieval, model=rag_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Question: What is ASCII?\n",
      "Answer: American Standard Code for Information Interchange\n",
      "\n",
      "==================================================\n",
      "Question: What is artificial intelligence?\n",
      "Answer: intelligence demonstrated by machines\n",
      "\n",
      "==================================================\n",
      "Question: Who was Albert Einstein?\n",
      "Answer: theoretical physicist\n",
      "\n",
      "==================================================\n",
      "Question: Where is Alabama located?\n",
      "Answer: Southeastern region of the United States\n",
      "\n",
      "==================================================\n",
      "Question: What is the function of an astronaut?\n",
      "Answer: serve as a commander or crew member aboard a spacecraft\n",
      "\n",
      "==================================================\n",
      "Question: What is the purpose of an albedo measurement?\n",
      "Answer: energy estimates\n",
      "\n",
      "==================================================\n",
      "Question: Who wrote Animal Farm?\n",
      "Answer: George Orwell\n"
     ]
    }
   ],
   "source": [
    "example_questions = [\n",
    "    \"What is ASCII?\",\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"Who was Albert Einstein?\",\n",
    "    \"Where is Alabama located?\",\n",
    "    \"What is the function of an astronaut?\",\n",
    "    \"What is the purpose of an albedo measurement?\",\n",
    "    \"Who wrote Animal Farm?\",\n",
    "]\n",
    "\n",
    "results = rag_pipeline.generate_answer(\n",
    "    question=example_questions,\n",
    "    top_k=cfg.RETRIEVE_TOP_K,\n",
    "    retrieve_distance_threshold=cfg.RETRIEVE_DISTANCE_THRESHOLD,\n",
    "    retrieve_min_k=cfg.RETRIEVE_MIN_K,\n",
    "    generation_config=generation_config,\n",
    "    batch_size=cfg.GENERATION_BATCH_SIZE,\n",
    ")\n",
    "\n",
    "for result in results:\n",
    "    question = result[\"question\"]\n",
    "    answer = result[\"answer\"]\n",
    "    print(f\"\\n\" + \"=\" * 50)\n",
    "    print(f\"Question: {question}\\nAnswer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bottleneck test\n",
    "If the vector db does not contain a relevant answer to the question, it will not be able to generate a good answer based on the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Question: What is RAG system in computer science? \n",
      "Answer: Structured systems analysis and design\n"
     ]
    }
   ],
   "source": [
    "q = \"What is RAG system in computer science?\"\n",
    "res = rag_pipeline.generate_answer(\n",
    "        question=q,\n",
    "        top_k=cfg.RETRIEVE_TOP_K,\n",
    "        retrieve_distance_threshold=cfg.RETRIEVE_DISTANCE_THRESHOLD,\n",
    "        retrieve_min_k=cfg.RETRIEVE_MIN_K,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "ans = res[\"answer\"]\n",
    "print(f\"\\n\" + \"=\" * 50)\n",
    "print(f\"Question: {q} \\nAnswer: {ans}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Performance and testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the RAG pipeline I used a test set, which contains questions about the wikipedia dataset.\n",
    "\n",
    "\"Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\"\n",
    "\n",
    "Dataset available: https://rajpurkar.github.io/SQuAD-explorer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file_path: str) -> Dict:\n",
    "    with open(cfg.SAVE_PATH + file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "titles = set(read_json(file_path= \"topics.json\")[\"topics\"])\n",
    "\n",
    "test_dataset = load_dataset(cfg.TEST_DATASET_NAME)\n",
    "\n",
    "def filter_by_topics(example: Dict, titles: List[str]) ->bool:\n",
    "    \"\"\"\n",
    "    Filter the titles. If the vector db does not contains relevant information about the given topic, the answer will not be good.\n",
    "    \"\"\"\n",
    "    return example[\"title\"] in titles\n",
    "\n",
    "def filter_by_topic_count(dataset, max_question_to_each_topic):\n",
    "    \"\"\"\n",
    "    Select 'max_question_to_each_topic' question about each topic.\n",
    "    \"\"\"\n",
    "    title_counts = defaultdict(int)\n",
    "    filtered_data = []\n",
    "\n",
    "    for example in dataset:\n",
    "        if title_counts[example[\"title\"]] < max_question_to_each_topic:\n",
    "            filtered_data.append(example)\n",
    "            title_counts[example[\"title\"]] += 1\n",
    "\n",
    "    return Dataset.from_list(filtered_data)\n",
    "\n",
    "test_dataset = test_dataset.filter(partial(filter_by_topics, titles=titles), num_proc=cfg.NUM_PROC)\n",
    "test_dataset = concatenate_datasets([test_dataset[\"train\"], test_dataset[\"validation\"]])\n",
    "test_dataset = filter_by_topic_count(test_dataset, cfg.MAX_QUESTION_TO_EACH_TOPIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate answers to all the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [00:21<00:00,  2.18s/it]\n"
     ]
    }
   ],
   "source": [
    "questions = test_dataset[\"question\"]\n",
    "test_generated_answers = rag_pipeline.generate_answer(\n",
    "    question=questions,\n",
    "    top_k=cfg.RETRIEVE_TOP_K,\n",
    "    retrieve_distance_threshold=cfg.RETRIEVE_DISTANCE_THRESHOLD,\n",
    "    retrieve_min_k=cfg.RETRIEVE_MIN_K,\n",
    "    generation_config=generation_config,\n",
    "    batch_size=cfg.GENERATION_BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = {}\n",
    "for test_data, test_gen in zip(test_dataset, test_generated_answers):\n",
    "    test_results[test_data[\"id\"]] = {\n",
    "        \"title\": test_data[\"title\"],\n",
    "        \"question\": test_data[\"question\"],\n",
    "        \"reference\": test_data[\"answers\"],\n",
    "        \"gt_context\": [test_data[\"context\"]],\n",
    "        \"response\": test_gen[\"answer\"],\n",
    "        \"retrieved_contexts\": test_gen[\"retrieved_texts\"]\n",
    "    }\n",
    "\n",
    "write_json(test_results, \"test_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the results using different metrics\n",
    "- squad_v2 metric: evaluates exact matching and f1-score\n",
    "- rouge metric: evaluates the overlap between generated text and reference text by comparing common n-grams, sequences, or words\n",
    "- meteor metric: evaluates the similarity between generated text and reference text by considering exact matches, synonyms, stemming, and word order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Amon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Amon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Amon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squad_v2: {'exact': 30.0, 'f1': 35.571022727272734, 'total': 80, 'HasAns_exact': 30.0, 'HasAns_f1': 35.571022727272734, 'HasAns_total': 80, 'best_exact': 30.0, 'best_exact_thresh': 0.0, 'best_f1': 35.571022727272734, 'best_f1_thresh': 0.0}\n",
      "==================================================\n",
      "rouge: {'rouge1': 0.35419084589684685, 'rouge2': 0.1375, 'rougeL': 0.3490970807514925, 'rougeLsum': 0.34859474129338924}\n",
      "==================================================\n",
      "meteor: {'meteor': 0.24793768635741764}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "GENERATION_METRICS = {\n",
    "    \"squad_v2\": evaluate.load(\"squad_v2\"),\n",
    "    \"rouge\": evaluate.load(\"rouge\", trust_remote_code=True),\n",
    "    \"meteor\": evaluate.load(\"meteor\", trust_remote_code=True)\n",
    "    }\n",
    "\n",
    "def calculate_metrics(metrics: Dict, results: Dict):\n",
    "    metrics_result = {}\n",
    "    pred_answers = [value[\"response\"] for key, value in results.items()]\n",
    "    gt_answers = [value[\"reference\"][\"text\"][0] for key, value in results.items()]\n",
    "    for metric_name, metric in metrics.items():\n",
    "        if \"squad_v2\" == metric.name:\n",
    "            predictions_squad_v2_format = [{'prediction_text': value[\"response\"], 'id': key, 'no_answer_probability': 0.} for key, value in results.items()]\n",
    "            references_squad_v2_format = [{\"answers\": value[\"reference\"], 'id': key} for key, value in results.items()]\n",
    "            metric_res = metric.compute(predictions=predictions_squad_v2_format, references=references_squad_v2_format)\n",
    "        else:\n",
    "            metric_res = metric.compute(predictions=pred_answers, references=gt_answers)\n",
    "        metrics_result[metric_name] = metric_res\n",
    "\n",
    "    return metrics_result\n",
    "\n",
    "test_generation_metrics_result = calculate_metrics(metrics=GENERATION_METRICS, results=test_results)\n",
    "write_json(test_results, \"test_generation_metrics_result.json\")\n",
    "for m_name, m_res in test_generation_metrics_result.items():\n",
    "    print(f\"{m_name}: {m_res}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test retrieval:\n",
    "\n",
    "To evaluate the retrieval, cosine similarity score was calculated between the ground truth context and the retrieved contexts.\n",
    "Then computes additional metrics such as average similarity, maximum similarity, and Precision@k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(gt_context: List, retrieved_contexts: List):\n",
    "    \"\"\"\n",
    "    Computes cosine similarity score between gt context and retrieved context.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for gt_cont, retrieved_cont in zip(gt_context, retrieved_contexts):\n",
    "        gt_embeddings = embedding_model.encode(gt_cont, convert_to_tensor=True)\n",
    "        tr = [\n",
    "            util.pytorch_cos_sim(\n",
    "                embedding_model.encode(ret_cont, convert_to_tensor=True),\n",
    "                gt_embeddings\n",
    "            ).mean().item()\n",
    "            for ret_cont in retrieved_cont\n",
    "        ]\n",
    "        results.append(tr)\n",
    "    return results\n",
    "\n",
    "def precision_at_k(cosine_similarities: List, threshold: float = 0.6):\n",
    "    \"\"\"\n",
    "    Computes precision based on cosine similarity scores using a specified threshold value.\n",
    "    \"\"\"\n",
    "    precisions = []\n",
    "    for sim_list in cosine_similarities:\n",
    "        relevant_in_k = sum(1 for sim in sim_list[:len(sim_list)] if sim > threshold)\n",
    "        precision = relevant_in_k / len(sim_list)\n",
    "        precisions.append(precision)\n",
    "    return sum(precisions) / len(precisions)\n",
    "\n",
    "def recall_at_k(cosine_similarities: List, threshold:float = 0.6):\n",
    "    \"\"\"\n",
    "    Computes recall based on cosine similarity scores using a specified threshold value.\n",
    "    \"\"\"\n",
    "    recalls = []\n",
    "    for sim_list in cosine_similarities:\n",
    "        total_relevant = sum(1 for sim in sim_list if sim > threshold)\n",
    "        relevant_in_k = sum(1 for sim in sim_list[:len(sim_list)] if sim > threshold)\n",
    "        recall = relevant_in_k / total_relevant if total_relevant > 0 else 0\n",
    "        recalls.append(recall)\n",
    "    return sum(recalls) / len(recalls)\n",
    "\n",
    "gt_context = [value[\"gt_context\"] for _, value in test_results.items()]\n",
    "retrieved_contexts = [value[\"retrieved_contexts\"] for _, value in test_results.items()]\n",
    "contexts_cosine_similarity = compute_cosine_similarity(gt_context, retrieved_contexts)\n",
    "\n",
    "max_similarity = [max(lst) for lst in contexts_cosine_similarity if len(lst) > 0]\n",
    "average_max_similarity = sum(max_similarity) / len(max_similarity)\n",
    "\n",
    "precision = precision_at_k(cosine_similarities=contexts_cosine_similarity, threshold=cfg.TEST_COS_SIM_THRESHOLD)\n",
    "recall = recall_at_k(cosine_similarities=contexts_cosine_similarity, threshold=cfg.TEST_COS_SIM_THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average max similarity: 0.7396492242813111\n",
      "Precision@5: 0.48625000000000007\n",
      "Recall@5: 0.825\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average max similarity: {average_max_similarity}\")\n",
    "print(f\"Precision@{cfg.RETRIEVE_TOP_K}: {precision}\")\n",
    "print(f\"Recall@{cfg.RETRIEVE_TOP_K}: {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8. Conclusion, summary**\n",
    "\n",
    "##### **Dataset**\n",
    "I have chosen the wikipedia dataset to create the RAG pipeline. It contains a huge amount of diverse data for the vector store.\n",
    "Following steps were applied:\n",
    "- removing irrelevant sections: remove irrelevant and unhelpful data that is just an overhead to the retrieval\n",
    "- splitting: long texts can not be fed to the embedding model well, there will be a data loss, then the generative model will not be able to handle due to context window limit.\n",
    "- cleaning: after the chunking there are many leftower tokens that are not useful. (Eg. chunk starts like: \". A ....\", where the \". \" is a placeholder)\n",
    "- removing duplicates: removing the duplicated chunks, to ensure each data is unique\n",
    "\n",
    "##### **Huggingface**\n",
    "I have chosen the Huggingface library to load models and create the RAG pipeline.\n",
    "Huggingface is an open-source library specialized to NLP tasks, offering a lot of useful API and infrastructure.\n",
    "It provides access to several models, datasets and tokenizers that are easy to use.\n",
    "The Huggingface library also has strong community support, and many fine tuned models are available for specialized tasks.\n",
    "\n",
    "##### **Embedding model**\n",
    "\n",
    "The embedded model I have chosen is: **multi-qa-distilbert-cos-v1**\n",
    "\n",
    "This model is fine-tuned on a QA dataset which makes it a good choice if the query will be a question. It is lightweight, making inference very fast.\n",
    "It is based on DistilBERT, which means the teacher network for this model was the BERT modela large, robust model with a significant number of parameters.\n",
    "Distillation refers to a process where the original BERT model acts as a teacher model, and a smaller model, with fewer parameters, is trained to replicate the predictions of the original BERT. According to the original paper, DistilBERT reduces the size of BERT by 40% while retaining 97% of its language understanding capabilities.\n",
    "The \"multi-qa-distilbert-cos-v1\" model is also specifically optimized for cosine similarity search, making it particularly useful as a retrieval.\n",
    "\n",
    "Original paper about distilbert: https://arxiv.org/pdf/1910.01108\n",
    "\n",
    "Model: https://huggingface.co/sentence-transformers/multi-qa-distilbert-cos-v1\n",
    "\n",
    "##### **Vector database**\n",
    "\n",
    "The vector database I have chosen is: **Faiss**\n",
    "\n",
    "Faiss is an extremely fast and efficient vector database, offering reduced memory usage compared to other vector stores. It is highly scalable and provides accurate vector search capabilities. While Faiss operates as an in-memory vector store (e.g., unlike Chroma, which is a persistent database), it can handle datasets that exceed the available RAM. Additionally, it supports both GPU and CPU for enhanced performance and flexibility.\n",
    "\n",
    "##### **Generative model**\n",
    "\n",
    "The vector database I have chosen is: **google/flan-t5-large**\n",
    "\n",
    "T5 is an encoder-decoder model, meaning the input is encoded (enabling better input understanding), and then the output is generated using cross attention between the input embeddings and output tokens. \n",
    "The model genretes the output in an autoregressive manner, with the decoder leveraging the encoded data for each token generation. \n",
    "\"flan\" refers to the model was fine-tuned on QA datasets and other specialized tasks (e.g., chain-of-thought reasoning) to enhance the ability to provide accurate ansers from the given context, making it ideal for QA pipelines. \n",
    "Encoder only models are not the best for generating anwsers based on context, they are better at classification tasks, like sentiment analysis.\n",
    "On the other hand, decoder only models are  effective for text generation tasks, but these models use only self attention to predict the next token based on the previously predicted tokens.\n",
    "However, if the question is too short, or the context is too large, the generated answer may lack faithfulness or accuracy.\n",
    "This is because these models do not use cross-attention, they only use self attention.\n",
    "\n",
    "Original paper: https://arxiv.org/pdf/2210.11416v5\n",
    "\n",
    "##### **Test**\n",
    "\n",
    "Test dataset: **rajpurkar/squad** (https://rajpurkar.github.io/SQuAD-explorer/)\n",
    "Available on Huggingface: https://huggingface.co/datasets/rajpurkar/squad\n",
    "\n",
    "SQuAD (Stanford Question Answering Dataset) contains questions and answers from wikipedia dataset.\n",
    "The question might be unanswerable. There is version differnce between SQuAD and the downloaded \"20220301.en\" dataset, so the context may not be the same to each question.\n",
    "Because the dataset which I used is not the whole wikipedia datasert, and the vector db does not contains all the necessary data to answer all the questions, filtering is applied to those topics which are in the vector store.\n",
    "\n",
    "**Retrieval test**\n",
    "\n",
    "To test the retrieval, based on the question and the ground truth context, the retrieved contexts are evaluated by cosine similarity and some other derived metrics namely, cosine similarity, recall@top_k, precision@top_k and average maximum similarity.\n",
    "- cosine similarity: measures the similarity between the retrieved context and the ground truth context by calculating the cosine of the angle between their vector representations. To enchance the accuracy normalization to the vectors are necessary.\n",
    "- recall@top_k: evaluates the percentage of relevant contexts that are present within the top-k retrieved results\n",
    "- precision@top_k: measures the proportion of retrieved contexts within the top-k results that are relevant\n",
    "- average maximum similarity: computes the average of the highest similarity scores between the retrieved contexts and the ground truth for each query\n",
    "\n",
    "\n",
    "**Generated answers test**\n",
    "To test the quality of the generated answers, I used SQuAD (Exact match, F1-score), ROUGE and METEOR metrics.\n",
    "- Exact match: measures the percentage of responses that match the ground truth answer exactly\n",
    "- F1-score: calculates the harmonic mean of precision and recall, considering partial matches\n",
    "- ROUGE: evaluates the overlap between the generated anwser and the reference text, focusing on the recall of n-grams, longest common subsequences, or word sequences\n",
    "- METEOR: measures the similarity between the generated text and the reference, accounting for synonyms, stemming, and word order flexibility\n",
    "\n",
    "**Other test metrics**\n",
    "\n",
    "There are other tool like RAGAS, DeepEval to evaluate the RAG system performance but these tools requires some extra steps like set API key.\n",
    "These tools can evaluate the whole RAG pipeline and calculates metrics like faithfulness, context recall, context precision, response relevancy and more.\n",
    "\n",
    "\n",
    "##### **Bottlenecks**\n",
    "- Data: preprocessing before embedding is a critical step. If the document contains lot of missinformation or badly structuralized documents, or even with duplicates, no matter the embedding model capability, both the embedding and retrieval process amy fail\n",
    "- Retrieve: if the retrieved contexts are not representative, the anser will not be helpful\n",
    "- Embedding: the embedding quality is key to retrieve the documents in a fast and efficient way. If the embeddings are not create good embedding, the retrieved documents can be different from the original topic\n",
    "- Hallucination: if the retrieved context is not informative or contains misinformation, the model can halucinate\n",
    "- Context window size: the Flan-T5-large model is a good choice to generate anwsers, but the context window is small: 512 tokens. Some questions may require long anwsers, and this system is not capable to do it, due to it's context window size\n",
    "\n",
    "##### **Future work**\n",
    "- improved data preprocessing: although, the data is preprocessed, but some other preprocessing methods can be applied.\n",
    "    - removing low-quality or redundant chunks by cosine similarity, L2 distance, embedding based similarity\n",
    "    - summarize long texts before chunking, this prevents the chunking method to split the paragraph into too many pieces, which are spearately meaningless\n",
    "    - more precise chunking: logically coherent chunking\n",
    "    - use text data cleaning tools like Cleanlab, SpaCy or NLTK\n",
    "- advanced prompting: the prompt is very clan and easy but with a more precise prompt the model may be able to generate better answers\n",
    "- fine-tuning: fine tune model on wikipedia QA dataset and fine-tuning the embedding model\n",
    "- hybrid retrieval methods: combine dense vector retrieval with sparse retrieval methods, like BM25 or DPR to impore retrieved documents relevancy\n",
    "- using advanced quantization techniques to load a bigger model\n",
    "- context summarization: after the retrieval summarize the documents to keep only the relevant part of the long context\n",
    "- using map reduce or refinement techniques to handle longer context and questions\n",
    "- more accurate testing: use some external tools, for example RAGAS or DeepEval\n",
    "    - LLM model testing: ROUGE, METEOR and SQuAD test are good, but these metrics are not reveals the contextual meanings.\n",
    "    - Retrieval testing: recall, precision and max similarity scores are a good baseline, but using rerank methods can be a big plus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
